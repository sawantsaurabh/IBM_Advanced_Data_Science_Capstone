{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Identify the Sentiments</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis is contextual mining of text which identifies and extracts subjective information in source material, and helping a business to understand the social sentiment of their brand, product or service while monitoring online conversations. Brands can use this data to measure the success of their products in an objective manner. In this project, you are provided with tweet data to predict sentiment on electronic products of netizens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType,IntegerType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pattern.en import sentiment, mood, modality\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=spark.read.csv(\"train_2kmZucJ - Copy.csv\",header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+\n",
      "| id|target|               tweet|\n",
      "+---+------+--------------------+\n",
      "|  1|     0|#fingerprint #Pre...|\n",
      "|  2|     0|Finally a transpa...|\n",
      "|  3|     0|We love this! Wou...|\n",
      "|  4|     0|I'm wired I know ...|\n",
      "|  5|     1|What amazing serv...|\n",
      "+---+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+-----------+----------+----------------+-------------+--------------+--------------------+------+-----------+------------------+--------------+\n",
      "| id|target|               tweet|comma_count|hash_count|exlametary_count|aderate_count|question_count|                text|length|words_count|words_not_stopword|stopword_count|\n",
      "+---+------+--------------------+-----------+----------+----------------+-------------+--------------+--------------------+------+-----------+------------------+--------------+\n",
      "|  1|     0|#fingerprint #Pre...|          0|        11|               0|            0|             0|fingerprint pregn...|   100|         13|                11|             2|\n",
      "|  2|     0|Finally a transpa...|          0|         5|               0|            0|             0|finally  transpar...|    89|         17|                14|             3|\n",
      "|  3|     0|We love this! Wou...|          0|         8|               1|            0|             1|we love this woul...|    92|         15|                10|             5|\n",
      "|  4|     0|I'm wired I know ...|          0|         4|               0|            0|             0|im wired  know im...|    74|         17|                15|             2|\n",
      "|  5|     1|What amazing serv...|          0|         0|               2|            0|             0|what amazing serv...|   112|         23|                14|             9|\n",
      "+---+------+--------------------+-----------+----------+----------------+-------------+--------------+--------------------+------+-----------+------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Wall time: 6.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stopWords = stopwords.words('english')+['mr', 'mrs', 'come', 'go', 'get',\n",
    "                             'tell', 'listen', 'one', 'two', 'three',\n",
    "                             'four', 'five', 'six', 'seven', 'eight',\n",
    "                             'nine', 'zero', 'join', 'find', 'make',\n",
    "                             'say', 'ask', 'tell', 'see', 'try', 'back',\n",
    "                             'also','oneplus','apple','android','iphone','samsung','sony']\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text) \n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopWords]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "def sent_TokenizeFunct(x):\n",
    "    return nltk.sent_tokenize(x)\n",
    "\n",
    "def word_TokenizeFunct(x):\n",
    "    splitted = [word for line in x for word in line.split()]\n",
    "    return splitted\n",
    "\n",
    "def removeStopWordsFunct(x):\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    filteredSentence = [w for w in x if not w in stop_words]\n",
    "    return filteredSentence\n",
    "\n",
    "\n",
    "sent_udf=udf(sent_TokenizeFunct)\n",
    "#sent_udf=udf(lambda x: sent_tokenize(x),StringType())\n",
    "word_udf=udf(word_TokenizeFunct)\n",
    "stop_udf=udf(remove_stopwords)\n",
    "#senti_udf=udf(lambda x: sentiment(x),StringType())\n",
    "#mod_udf=udf(lambda x: modality(x),StringType())\n",
    "#upp_udf=udf(lambda x: len([wrd for wrd in x.split() if wrd.isupper()],StringType())\n",
    "commas_udf=udf(lambda x: x.count(','),IntegerType())\n",
    "hash_udf=udf(lambda x: x.count('#'),IntegerType())\n",
    "exlametary_udf=udf(lambda x: x.count('!'),IntegerType())\n",
    "aderate_udf=udf(lambda x: x.count('@'),IntegerType())\n",
    "question_udf=udf(lambda x: x.count('?'),IntegerType())\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lem_udf= udf(lambda x: ' '.join([wordnet_lemmatizer.lemmatize(l) for l in x.split()]),StringType())\n",
    "words_only_udf=udf(lambda x: re.sub(r'[^\\w\\s]','', x.lower()),StringType())\n",
    "single_words_udf=udf(lambda x: re.sub(r'\\b\\w{1,1}\\b', '', x.lower()),StringType())\n",
    "length_udf = udf(lambda x: len(x),IntegerType())\n",
    "#get number of words\n",
    "words_count_udf = udf(lambda x: len(x.split(' ')),IntegerType())\n",
    "#df['word_density'] = df['length'] / (df['words']+1)\n",
    "words_not_stopword_udf = udf(lambda x: len([t for t in x.split(' ') if t not in stopWords]),IntegerType())\n",
    "#get the average word length\n",
    "avg_word_length_udf = udf(lambda x: np.mean([len(t) for t in x.split(' ') if t not in stopWords]) if len([len(t) for t in x.split(' ') if t not in stopWords]) > 0 else 0,IntegerType())\n",
    "stopword_count_udf = udf(lambda x: len([wrd for wrd in x.split() if wrd.lower() in stopWords]),IntegerType())\n",
    "    \n",
    "    \n",
    "train_df_1= train.withColumn('tweet', regexp_replace('tweet', '$&@*#', 'profaneword'))\n",
    "train_df_1= train_df_1.withColumn('tweet', regexp_replace('tweet', \"http\\S+\", \"link\"))\n",
    "train_df_1=train_df_1.withColumn(\"comma_count\", commas_udf(\"tweet\"))\n",
    "train_df_1=train_df_1.withColumn(\"hash_count\", hash_udf(\"tweet\"))\n",
    "train_df_1=train_df_1.withColumn(\"exlametary_count\", exlametary_udf(\"tweet\"))\n",
    "train_df_1=train_df_1.withColumn(\"aderate_count\", aderate_udf(\"tweet\"))\n",
    "train_df_1=train_df_1.withColumn(\"question_count\", question_udf(\"tweet\"))\n",
    "\n",
    "train_df_1=train_df_1.withColumn(\"text\", lem_udf(\"tweet\"))\n",
    "train_df_1=train_df_1.withColumn(\"text\", words_only_udf(\"text\"))\n",
    "train_df_1=train_df_1.withColumn(\"text\", single_words_udf(\"text\"))\n",
    "train_df_1= train_df_1.withColumn('text', regexp_replace('text', \"\\d+\", \"\"))\n",
    "train_df_1=train_df_1.withColumn(\"length\", length_udf(\"text\"))\n",
    "train_df_1=train_df_1.withColumn(\"words_count\", words_count_udf(\"text\"))\n",
    "#train_df_1=train_df_1.withColumn(\"words_density\",train_df_1.length /train_df_1.length.words)\n",
    "train_df_1=train_df_1.withColumn(\"words_not_stopword\", words_not_stopword_udf(\"text\"))\n",
    "#train_df_1=train_df_1.withColumn(\"avg_word_length\", avg_word_length_udf(\"text\"))\n",
    "train_df_1=train_df_1.withColumn(\"stopword_count\", stopword_count_udf(\"text\"))\n",
    "\n",
    "\n",
    "train_df_1.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"target\", outputCol=\"label\")\n",
    "indexed = indexer.fit(train_df_1).transform(train_df_1)\n",
    "#indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2=indexed.select([c for c in indexed.columns if c not in ['id','target','tweet']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_set, val_set, test_set) = train_df_2.randomSplit([0.98, 0.01, 0.01], seed = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score: 0.8824\n",
      "SVM ROC-AUC: 0.9510\n",
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "\n",
    "lsvc=LinearSVC(maxIter=5)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf,lsvc])\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "predictions = pipelineFit.transform(val_set)\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"SVM Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print(\"SVM ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Accuracy Score: 0.8235\n",
      "Logistic ROC-AUC: 0.8781\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(maxIter=100)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf,lr])\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "predictions = pipelineFit.transform(val_set)\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Logistic Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print(\"Logistic ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
